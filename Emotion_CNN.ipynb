{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50e479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D,MaxPool2D, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "#import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc65c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train/'\n",
    "val_dir = 'test/'\n",
    "batch_size = 32\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6728bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "     zoom_range = 0.2, \n",
    "     shear_range = 0.2, \n",
    "     width_shift_range=0.2,\n",
    "     rotation_range=10,\n",
    "     height_shift_range=0.2,\n",
    "     horizontal_flip=True, \n",
    "     rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fa42a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb909641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
    "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d83b78c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "897/897 [==============================] - 54s 48ms/step - loss: 8.9326 - accuracy: 0.2117 - val_loss: 7.8151 - val_accuracy: 0.2786 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "897/897 [==============================] - 36s 40ms/step - loss: 7.1547 - accuracy: 0.2745 - val_loss: 6.2021 - val_accuracy: 0.3073 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 5.4784 - accuracy: 0.3350 - val_loss: 4.6348 - val_accuracy: 0.4022 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "897/897 [==============================] - 41s 45ms/step - loss: 4.1822 - accuracy: 0.3846 - val_loss: 3.5678 - val_accuracy: 0.4135 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 3.2383 - accuracy: 0.4353 - val_loss: 2.8500 - val_accuracy: 0.4534 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 2.6191 - accuracy: 0.4759 - val_loss: 2.3927 - val_accuracy: 0.5028 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 2.2512 - accuracy: 0.5075 - val_loss: 2.0876 - val_accuracy: 0.5335 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "897/897 [==============================] - 38s 42ms/step - loss: 2.0143 - accuracy: 0.5289 - val_loss: 1.8690 - val_accuracy: 0.5618 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "897/897 [==============================] - 38s 42ms/step - loss: 1.8588 - accuracy: 0.5492 - val_loss: 1.7880 - val_accuracy: 0.5691 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 1.7581 - accuracy: 0.5643 - val_loss: 1.6771 - val_accuracy: 0.5963 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "897/897 [==============================] - 38s 43ms/step - loss: 1.6796 - accuracy: 0.5910 - val_loss: 1.6527 - val_accuracy: 0.6017 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "897/897 [==============================] - 38s 43ms/step - loss: 1.6340 - accuracy: 0.6022 - val_loss: 1.6634 - val_accuracy: 0.5946 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 1.5932 - accuracy: 0.6204 - val_loss: 1.6423 - val_accuracy: 0.6091 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "897/897 [==============================] - 39s 44ms/step - loss: 1.5733 - accuracy: 0.6333 - val_loss: 1.6508 - val_accuracy: 0.6069 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "897/897 [==============================] - 38s 43ms/step - loss: 1.5594 - accuracy: 0.6490 - val_loss: 1.6765 - val_accuracy: 0.6166 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "897/897 [==============================] - 38s 43ms/step - loss: 1.5434 - accuracy: 0.6635 - val_loss: 1.6944 - val_accuracy: 0.6215 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "897/897 [==============================] - 39s 43ms/step - loss: 1.5352 - accuracy: 0.6737 - val_loss: 1.7044 - val_accuracy: 0.6265 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "897/897 [==============================] - 35s 39ms/step - loss: 1.5254 - accuracy: 0.6887 - val_loss: 1.7481 - val_accuracy: 0.6292 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 1.5139 - accuracy: 0.7034 - val_loss: 1.7667 - val_accuracy: 0.6370 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 1.5104 - accuracy: 0.7137 - val_loss: 1.8524 - val_accuracy: 0.6115 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 1.5004 - accuracy: 0.7282 - val_loss: 1.8358 - val_accuracy: 0.6352 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 1.5045 - accuracy: 0.7393 - val_loss: 1.8759 - val_accuracy: 0.6324 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 1.5057 - accuracy: 0.7492 - val_loss: 1.9323 - val_accuracy: 0.6346 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "897/897 [==============================] - ETA: 0s - loss: 1.4983 - accuracy: 0.7598Restoring model weights from the end of the best epoch: 19.\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 1.4983 - accuracy: 0.7598 - val_loss: 1.9667 - val_accuracy: 0.6357 - lr: 1.0000e-04\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Compiling the Model\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta= 0.01, patience=5, verbose=1, restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('vgg16_model.h5', save_best_only=True, monitor='val_accuracy')\n",
    "red_lr = ReduceLROnPlateau(monitor=\"val_accuracy\", patience=3, verbose=1, factor = 0.1, min_lr=0.001)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // batch_size,\n",
    "        validation_steps=7178 // batch_size,\n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        batch_size = 64,\n",
    "        callbacks=[red_lr,earlyStopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b21cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1093cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac41fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de70ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624454dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3380c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
